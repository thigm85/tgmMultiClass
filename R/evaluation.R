#' Check match of the resample indexes and prediction tags
#' 
#' @return In case there is a mismatch it will give you a warning if
#' \code{ignore_tag = TRUE} or trigger an \code{stop} in case 
#' \code{ignore_tag = FALSE}.
#' 
#' @export
checkTags <- function(resample_tag, prediction_tag, ignore_tag){
  if (resample_tag != prediction_tag){
    if (ignore_tag){
      warning("Careful: There is a mismatch between prediction and resample tags.")
    } else {
      stop("There is a mismatch between prediction and resample tags.")
    }
  } 
}

#' Compute the Log Score of predictions
#' 
#' Compute the log score from multi-class probability predictions.
#' 
#' @details Given a set of \code{prediction} and \code{target} values it will
#'   compute the log score function for each prediction point.
#'   
#' @param prediction Should be a matrix where the number of columns is equal to
#'   the number of classes in the problem and each column contains the
#'   probability of a specific class being true, each row should sum to 1.
#' @param target Should be a matrix with same dimension as \code{prediction} 
#'   where each cell of the matrix count the number of successes of that 
#'   particular class within that observational unit. Each cell of the matrix 
#'   should be an integer greater or equal to zero.
#'   
#' @return Numeric vector with length equal to the number of rows of
#' \code{prediction} and \code{target} containing the log scores for each
#' prediction point.
#' 
#' @export       
computeLogScore <- function(prediction, target){
  as.numeric(rowSums(log(prediction) * target))
}

#' Compute scores from multi-class probability predictions.
#' 
#' @details Given a set of \code{prediction} and \code{target} values it will
#'   compute the score function for each prediction point.
#'   
#' @param prediction Should be a matrix where the number of columns is equal to
#'   the number of classes in the problem and each column contains the
#'   probability of a specific class being true, each row should sum to 1.
#' @param target Should be a matrix with same dimension as \code{prediction} 
#'   where each cell of the matrix count the number of successes of that 
#'   particular class within that observational unit. Each cell of the matrix 
#'   should be an integer greater or equal to zero.
#' @param type Which score function to use. 
#' 
#' @details Currently available score functions are 'log_score'.
#'   
#' @return Numeric vector with length equal to the number of rows of
#' \code{prediction} and \code{target} containing the scores for each
#' prediction point.
#' 
#' @export       
computeMultiClassScore <- function(prediction, target, type, ...){
  
  if (type == "log_score"){
    result <- computeLogScore(prediction = prediction, target = target)
    return(result)
  } else {
    stop("Invalid type.\n")
  }
  
}

# #' Compute prediction log scores for a \code{multiClass} object.
# #' 
# #' @details Given a \code{datasetResample} object and a compatible 
# #' \code{multiClass} prediction object it compute log scores of the
# #' predictions for each replication.
# #' 
# #' @inheritParams evaluateProbClass
# #'  
# #' @return A matrix where each column represents the prediction log scores
# #' for a given replication. The number of rows is equal to the number
# #' of prediction points.
# computeLogScoreMultiClass <- function(resample_indexes, pred_obj){
#   
#   number_replicates <- mcGet(resample_indexes, "number_replicates")
#   scores_all <- list()
#   
#   for (i in 1:number_replicates){
#     
#     predictions_i <- mcGet(pred_obj, "prob", i)
#     target_i <- mcGet(resample_indexes, "test_target", i)
#     scores_all[[i]] <- computeLogScore(prediction = predictions_i, target = target_i)
#     
#   }
#   
#   class(scores_all) <- "multiClassScores"
#   return(scores_all)
#   
# } 

#' Constructor for \code{multiClassScores} objects
#' 
#' \code{multiClassScores} objects hold Scores obtained from comparing prediction with test set observations.
#' 
#' \code{multiClassScores} objects hold Scores obtained by comparing predictions performed
#' over a series of test sets with actual observed values. A tag exist
#' to connect the scores with the object that contains information about
#' the selected test sets. This allows results from different models to be 
#' compared since two \code{multiClassScores} objects with the same tag are guaranteed 
#' to be generated from the same test set points.
#' 
#' @param scores_list List where each element contains a numerical vector with the score
#'  values for each prediction point in a specific test set.
#' @param resample_indexes S3 object of class \code{datasetResample} with information
#'  about Class Labels and the data used using for training and predictions. Usually
#'  generated by the \code{generateTestIndexes} function.
#' @param score_type Which type of score function was used.
#' 
#' @return S3 object of class \code{multiClassScores}.
#' 
#' @family tgmMultiClass constructors
#' @seealso \code{\link{generateTestIndexes}}, 
#'  \code{\link{summaryValidation}}, 
#'  \code{\link{evaluateProbClass}}
#'
#' @export
multiClassScores <- function(scores_list, resample_indexes, score_type){
  
  result <- list(scores_list = scores_list,
                 score_type = score_type,
                 resample_tag = mcGet(resample_indexes, "tag"))
  class(result) <- "multiClassScores"
  
  return(result)
  
}

#' Extract info from \code{multiClassScores} objects
#' 
#' @export
mcGet.multiClassScores <- function(x, attr, i = NULL){
  
  if (attr == "scores_list"){ # score values
    
    if (is.null(i)){
      return(x[["scores_list"]])  
    } else {
      if (i >= 1 & i <= length(x[["scores_list"]])){
        result <- x[["scores_list"]][[i]]
        return(result)
      } else {
        stop("index i out of range.")
      }
    }
    
  } else if (attr == "score_type"){ # type of the score used
    
    return(x[["score_type"]])
    
  } else if (attr == "tag"){ # resample indexes tag
    
    return(x[["resample_tag"]])
    
  } else {
    stop(attr, " not found.")
  }
  
}

#' Evaluate predicted class probabilities for a given score function.
#' 
#' @details Given a \code{datasetResample} object and a compatible 
#' \code{multiClass} prediction object it computes prediction scores
#' of some \code{type} for each replication. 
#' 
#' @param pred_obj A \code{multiClass} object, usually generated
#'  from the \code{datasetResample} object with
#'  some predictive model with an interface function to 
#'  the \code{tgmMultiClass} package.
#' @param resample_indexes A \code{datasetResample} object, usually
#'  generated by the \code{generateTestIndexes} function.
#' @param type Which type of score function should be used. Currently,
#'  on \code{log_score} is available.
#' @param ignore_tag Before computing scores, we check to see if 
#'  \code{pred_obj} was indeed generated from the \code{resample_indexes}.
#'  In case of a mismatch an error is issued, unless \code{ignore_tag = FALSE},
#'  in which case just a warning is issued.
#' 
#' @return An S3 object of class \code{multiClassScores}.
#' 
#' @export
evaluateProbClass.multiClass <- function(pred_obj, resample_indexes, type, ignore_tag = FALSE, ...){
  
  resample_tag <- mcGet(resample_indexes, "tag")
  prediction_tag <- mcGet(pred_obj, "tag")
  
  checkTags(resample_tag = resample_tag, 
            prediction_tag = prediction_tag, 
            ignore_tag = ignore_tag)
    
  number_replicates <- mcGet(resample_indexes, "number_replicates")
  scores_all <- list()
  
  for (i in 1:number_replicates){
    
    predictions_i <- mcGet(pred_obj, "prob", i)
    target_i <- mcGet(resample_indexes, "test_target", i)
    scores_all[[i]] <- computeMultiClassScore(prediction = predictions_i, target = target_i, type = type, ...)
    
  }

  result <- multiClassScores(scores_list = scores_all, 
                             resample_indexes = resample_indexes, 
                             score_type = type)
  return(result)
  
}

#' Constructor for \code{multiClassValidationScores} objects
#' 
#' \code{multiClassValidationScores} objects hold Scores obtained from 
#' comparing multiple model predictions to a specific validation set.
#' A tag and replication index exist to connect the scores with the 
#' object that contains information about the selected validation set. 
#' This allow us to identify what was the exact validation set used to 
#' produce the scores.
#' 
#' @param scores_list List where each element contains a numerical vector with the score
#'  values for each prediction point generated by a specific model for a specific 
#'  validation set.
#' @param resample_indexes S3 object of class \code{datasetResample} with information
#'  about Class Labels and the data used using for training and predictions. Usually
#'  generated by the \code{generateTestIndexes} function.
#' @param replicate_index Integer number that indicates which validation set was used.
#' @param score_type Which type of score function was used.
#' @param tune_grid It is a matrix with named columns containing tuning parameters of the
#'  model. Each row of the matrix represents tuning parameter values of a specific model.
#' 
#' @return S3 object of class \code{multiClassValidationScores}.
#' 
#' @family tgmMultiClass constructors
#' @seealso \code{\link{generateTestIndexes}}, 
#'  \code{\link{summaryValidation}}, 
#'  \code{\link{evaluateProbClass}}
#' @export  
multiClassValidationScores <- function(scores_list, resample_indexes, replicate_index, 
                                       score_type, tune_grid){
  
  result <- list(scores_list = scores_list,
                 score_type = score_type,
                 replicate_index = replicate_index,
                 resample_tag = mcGet(resample_indexes, "tag"),
                 tune_grid = tune_grid)
  class(result) <- "multiClassValidationScores"
  
  return(result)
  
}

#' Extract info from \code{multiClassValidationScores} objects
#' 
#' @export
mcGet.multiClassValidationScores <- function(x, attr, i = NULL){
  
  if (attr == "scores_list"){ # score values
    
    if (is.null(i)){
      return(x[["scores_list"]])  
    } else {
      if (i >= 1 & i <= length(x[["scores_list"]])){
        result <- x[["scores_list"]][[i]]
        return(result)
      } else {
        stop("index i out of range.")
      }
    }
    
  } else if (attr == "score_type"){ # type of the score used
    
    return(x[["score_type"]])
    
  } else if (attr == "replicate_index"){ # which validation set was used
    
    return(x[["replicate_index"]])
    
  } else if (attr == "tag"){ # resample indexes tag
    
    return(x[["resample_tag"]])
    
  } else if (attr == "tune_grid"){ # resample indexes tag
    
    return(x[["tune_grid"]])
    
  } else {
    stop(attr, " not found.")
  }
  
}

#' Evaluate predicted class probabilities over a validation set for a given score function.
#' 
#' @details Given a \code{datasetResample} object and a compatible 
#' \code{multiClassValidation} prediction object it computes prediction scores
#' of some \code{type} for each model used in the validation step. 
#' 
#' @param pred_obj A \code{multiClassValidation} object, usually generated
#'  from the \code{datasetResample} object with
#'  some set of predictive models through an interface function to 
#'  the \code{tgmMultiClass} package.
#' @param resample_indexes A \code{datasetResample} object, usually
#'  generated by the \code{generateTestIndexes} function.
#' @param type Which type of score function should be used. Currently,
#'  on \code{log_score} is available.
#' @param ignore_tag Before computing scores, we check to see if 
#'  \code{pred_obj} was indeed generated from the \code{resample_indexes}.
#'  In case of a mismatch an error is issued, unless \code{ignore_tag = FALSE},
#'  in which case just a warning is issued.
#' 
#' @return An S3 object of class \code{multiClassValidationScores}.
#' 
#' @export
evaluateProbClass.multiClassValidation <- function(pred_obj, resample_indexes, type, 
                                                   ignore_tag = FALSE, replicate_index = NULL, ...){
  
  resample_tag <- mcGet(resample_indexes, "tag")
  prediction_tag <- mcGet(pred_obj, "tag")
  
  checkTags(resample_tag = resample_tag, 
            prediction_tag = prediction_tag, 
            ignore_tag = ignore_tag)
  
  number_models <- mcGet(pred_obj, "number_models")
  scores_all <- list()
  
  for (i in 1:number_models){
    
    predictions_i <- mcGet(pred_obj, "prob", i)
    target_i <- mcGet(resample_indexes, "validation_target", i = replicate_index)
    scores_all[[i]] <- computeMultiClassScore(prediction = predictions_i, target = target_i, type = type, ...)
    
  }
  
  result <- multiClassValidationScores(scores_list = scores_all, 
                                       resample_indexes = resample_indexes, 
                                       replicate_index = replicate_index, 
                                       score_type = type,
                                       tune_grid = mcGet(pred_obj, "tune_grid"))
  return(result)
  
}



#' Summarize Validation metric
#' 
#' @export
summarizeValidationMetric.multiClassValidationScores <- function(validation_scores, 
                                                                 previous_summary = NULL, 
                                                                 summarize_function = function(x) mean(x, na.rm = TRUE), 
                                                                 ...){
  
  replicate_index <- mcGet(validation_scores, "replicate_index")
  tune_grid <- mcGet(validation_scores, "tune_grid")
  scores_list <- mcGet(validation_scores, "scores_list")
  
  new_matrix <- cbind(replicate_index, tune_grid, 
                      as.numeric(sapply(scores_list, FUN = summarize_function)))
  colnames(new_matrix) <- c("Replicate", colnames(tune_grid), "metric")
  
  summary <- rbind(previous_summary, 
                   new_matrix)
  #class(summary) <- "summaryValidation"
  
  return(summary)
  
}

#' Select the best model based on computed metric
#' 
#' @export
pickBestValidationModel <- function(validation_scores, 
                                    summarize_function = function(x) mean(x, na.rm = TRUE)){
  
  scores_list <- mcGet(validation_scores, "scores_list")
  
  best_model <- which.max(as.numeric(sapply(scores_list, FUN = summarize_function)))
  return(best_model)
  
}


#' Summarize results from the validation step
#' 
#' @export
summarizeValidation.multiClass <- function(fitted_model, ...){

  if (!require(ggplot2)) stop("Please, install ggplot2.")
  
  summary_validation <- mcGet(fitted_model, "summary_validation")
  
  if (is.null(summary_validation)){
    return(NULL)    
  }
  
  parameter_names <- mcGet(fitted_model, "summary_validation", i = "parameter_names") 
  
  aggregated_over_replicates <- summary_validation %>%
    s_group_by(parameter_names) %>%
    summarise(metric = mean(metric, na.rm = TRUE))
  
  plots <- list()
  optimal_parameters <- NULL
  joint_data_to_plot <- NULL
  
  count <- 0
  
  for (parameter in parameter_names){
    
    count <- count + 1
    
    data_to_plot <- aggregated_over_replicates %>% 
      s_group_by(parameter) %>%
      summarise(metric = max(metric, na.rm = TRUE))
    original_colnames <- colnames(data_to_plot)
    colnames(data_to_plot) <- c("x", "y")
    
    plot_obj <- ggplot(data_to_plot) + 
      geom_line(aes(x = x, y = y)) + 
      geom_point(aes(x = x, y = y)) +
      labs(x = original_colnames[1], y = original_colnames[2], title = parameter)
    
    optimal_value <- data_to_plot[which.max(x = data_to_plot[,2]), 1]
    
    plots[[count]] <- plot_obj
    optimal_parameters <- c(optimal_parameters, optimal_value)
    
    joint_data_to_plot <- rbind(joint_data_to_plot, cbind(parameter, data_to_plot))
    
  }
  
  names(optimal_parameters) <- parameter_names
  
  joint_plot <- ggplot(joint_data_to_plot) + 
    geom_line(aes(x = x, y = y)) + 
    geom_point(aes(x = x, y = y)) +
    facet_wrap( ~ parameter, scales = "free_x")
  
  result <- list(plots = plots,
                 optimal_parameters = optimal_parameters,
                 parameter_names = parameter_names,
                 joint_plot = joint_plot)
  class(result) <- "summarizedValidation"
  
  return(result)
  
}

#' Get elements from \code{summarizedValidation} objects
#' 
#' @export
mcGet.summarizedValidation <- function(x, attr, ...){
  if (attr == "joint_plot"){ # Joint plot of the parameters
    return(x[["joint_plot"]])
  } else if (attr == "optimal_parameters"){ # Optimal parameters
    return(x[["optimal_parameters"]])
  } else {
    stop(attr, " not found.")
  }
}

#' Plot method for \code{summarizedValidation} class objects
#' 
#' @export
plot.summarizedValidation <- function(x, y = NULL, ...){
  print(mcGet(x, "joint_plot"))
}

#' Print method for \code{summarizedValidation} class objects
#' 
#' @export
print.summarizedValidation <- function(x, ...){
  print(mcGet(x, "optimal_parameters"))
}

#' Summary method for \code{summarizedValidation} class objects
#' 
#' @export
summary.summarizedValidation <- function(object, ...){
  print(object)
  plot(object)
}



## Transform the function below into a method. We have an equivalent one for uniCont

#' Build calibration plots for a \code{multiClass} object
#' 
#' @export
checkCalibration <- function(fitted_model, resample_indexes, number_bins, order_smooth = 2){
  
  if (!(inherits(fitted_model, "multiClass") & 
          inherits(resample_indexes, "datasetResample"))){
    stop("'fitted_model' needs to be a 'multiClass' object and 'resample_indexes' needs to be a 'datasetResample'")
  }
  
  if (!require(ggplot2)) stop("Please, install ggplot2.")
  if (!require(MASS)) stop("Please, install MASS.")
  if (!require(splines)) stop("Please, install splines.")
  
  number_replicates <- mcGet(fitted_model, "number_replicates")
  
  joint_pred_probs <- NULL
  joint_target <- NULL
  for (i in 1:number_replicates){ 
    joint_pred_probs <- rbind(joint_pred_probs, mcGet(fitted_model, "prob", i))
    joint_target <- rbind(joint_target, mcGet(resample_indexes, "test_target", i))
  }
  
  number_classes <- mcGet(fitted_model, "number_classes")
  
  calibration_objects <- list()
  plot_calibration_points <- list()
  plot_smooth_calibration <- list()
  for (i in 1:number_classes){
    
    pred_prob_i <- joint_pred_probs[, i]
    
    cuts <- quantile(x = pred_prob_i, probs = seq(0, 1, length.out = number_bins + 1))
    pred_prob_bins <- cut(pred_prob_i, breaks = unique(cuts))
    pred_points <- tapply(pred_prob_i, pred_prob_bins, mean, na.rm=TRUE)
    
    bin_sums <- NULL
    for (j in 1:number_classes){
      bin_sums <- cbind(bin_sums, tapply(joint_target[, j], pred_prob_bins, sum, na.rm=TRUE))  
    }
    
    
    calibration_objects[[i]] <- data.frame(prob_pred = pred_points, 
                                           empirical_prob = bin_sums[,i]/rowSums(bin_sums))  
    
    
    plot_calibration_points[[i]] <- ggplot(calibration_objects[[i]]) + 
      geom_point(aes(x = prob_pred, y = empirical_prob)) + 
      labs(x = "Predicted Prob.", y = "Empirical Prob.") +
      geom_abline(intercept = 0, slope = 1)
    
    data_to_plot <- data.frame(pred = joint_pred_probs[,i], obs = joint_target[,i])
    plot_smooth_calibration[[i]] <- ggplot(data_to_plot, aes(x = pred, y = obs)) + 
      stat_smooth(method = "glm", formula = y ~ ns(x, order_smooth), family = "binomial") + 
      xlim(range(calibration_objects[[i]][, "prob_pred"])) +
      geom_abline(intercept = 0, slope = 1) +
      geom_point(data = calibration_objects[[i]], mapping = aes(x = prob_pred, empirical_prob)) + 
      labs(x = "Predicted Prob.", y = "Empirical Prob.")
    
  }
  
  result <- list(calibration_objects = calibration_objects,
                 class_labels = mcGet(resample_indexes, "class_labels"),
                 plot_calibration_points = plot_calibration_points,
                 plot_smooth_calibration = plot_smooth_calibration)
  class(result) <- "multiClass_calibration"
  
  return(result)
  
}

#' Build calibration plots given x- and and y- axis
#' 
#' @param x probability vector for the class of interst
#' @param y Target matrix with K columns for K class problem
#' @param which_class which class should be plotted.
#' 
#' @export
checkCalibrationBaseProb <- function(x, y, number_bins, which_class, order_smooth = 2){
  
  if (!require(ggplot2)) stop("Please, install ggplot2.")
  if (!require(MASS)) stop("Please, install MASS.")
  if (!require(splines)) stop("Please, install splines.")
  
#   joint_pred_probs <- NULL
#   joint_target <- NULL
#   for (i in 1:number_replicates){ 
#     joint_pred_probs <- rbind(joint_pred_probs, mcGet(fitted_model, "prob", i))
#     joint_target <- rbind(joint_target, mcGet(resample_indexes, "test_target", i))
#   }
#   
#   number_classes <- mcGet(fitted_model, "number_classes")
#   
#   calibration_objects <- list()
#   plot_calibration_points <- list()
#   plot_smooth_calibration <- list()
#   for (i in 1:number_classes){
    
#     pred_prob_i <- joint_pred_probs[, i]
    
    cuts <- quantile(x = x, probs = seq(0, 1, length.out = number_bins + 1))
    x_bins <- cut(x, breaks = unique(cuts))
    x_points <- tapply(x, x_bins, mean, na.rm=TRUE)
    
    number_classes <- ncol(y)

    bin_sums <- NULL
    for (j in 1:number_classes){
      bin_sums <- cbind(bin_sums, tapply(y[, j], x_bins, sum, na.rm=TRUE))  
    }
    
    
    calibration_objects <- data.frame(prob_pred = x_points, 
                                      empirical_prob = bin_sums[,which_class]/rowSums(bin_sums))  
    
    
    plot_calibration_points <- ggplot(calibration_objects) + 
      geom_point(aes(x = prob_pred, y = empirical_prob)) + 
      labs(x = "Predicted Prob.", y = "Empirical Prob.") +
      geom_abline(intercept = 0, slope = 1)
    
    data_to_plot <- data.frame(pred = x, obs = y[,which_class])
    plot_smooth_calibration <- ggplot(data_to_plot, aes(x = pred, y = obs)) + 
      stat_smooth(method = "glm", formula = y ~ ns(x, order_smooth), family = "binomial") + 
      xlim(range(calibration_objects[, "prob_pred"])) +
      geom_abline(intercept = 0, slope = 1) +
      geom_point(data = calibration_objects, mapping = aes(x = prob_pred, empirical_prob)) + 
      labs(x = "Predicted Prob.", y = "Empirical Prob.")
    
#   }
  
  result <- list(calibration_objects = calibration_objects,
                 plot_calibration_points = plot_calibration_points,
                 plot_smooth_calibration = plot_smooth_calibration)
  
  return(result)
  
}
